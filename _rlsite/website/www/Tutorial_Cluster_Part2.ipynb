{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Part 2: Good practices'\n",
        "format: html\n",
        "execute:\n",
        "  freeze: false\n",
        "  eval: false\n",
        "author: Romain Ligneul\n",
        "---\n",
        "\n",
        "{{< downloadthis \"www/Tutorial_Cluster_Part2.ipynb\" label=\"Download this notebook\" >}}\n",
        "\n",
        ":::{.callout-note}\n",
        "If you are working on the CRNL cluster, you can find also the corresponding notebook at this location: `/crnldata/projets_communs/tutorials/`\n",
        ":::\n",
        "\n",
        "If you managed to complete the first part of this tutorial, you will also be able to `pip install` whatever in your virtual environment and do some computing.\n",
        "However, there is more to know.\n",
        "\n",
        "\n",
        "## Sharing the resources\n",
        "\n",
        "Because it needs to remain highly flexible and adapted to a wide range of needs, the cluster is not very constrained with respect to resource allocation.\n",
        "\n",
        "If you do not pay attention, you might monopolize all the CPUs or all the memory with your jobs, without leaving anything behind for your colleagues.\n",
        "\n",
        "That why evaluating the amount of memory you need and the maximum time that a non-bugged job might take is important! Based on this information, you can adjust `mem_gb` and `timeout_min` (timeout in minutes) well.\n",
        "\n",
        "Similarly, you may need to decide how many CPUs will be useful for you. Can you go with only one without losing much? Then use only 1. Do you divide your computation time by a huge factor if you use more, then use more. But how will you know?\n",
        "\n",
        " What follows should help you with all this.\n",
        "\n",
        "### Anticipating time-memory consumption\n",
        "\n",
        "Hereafter, we use memory_usage, which has a slighty unusual way of passing arguments to its target function.\n",
        "All positional arguments (those without an = sign in the *def*) are passed together, and all non-positional arguments (also called key-pairs) are passed together.\n",
        "For example, we could try:<br>\n",
        "`mem_usage=memory_usage((somefunc,(0.1,4,0.88), {'file' : 'whatever.csv','index' : 0 }))` <br>\n",
        "If we had a function defined like this: <br>\n",
        "`somefunc(a,b,c, file=None, index=-1)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###### simple memory/time check\n",
        "from memory_profiler import memory_usage\n",
        "import time\n",
        "\n",
        "# define a single thread function\n",
        "def duplicate_ones(a, n=100, x=0):\n",
        "    import time\n",
        "    time.sleep(1)\n",
        "    b = [a] * n\n",
        "    b = [a] * n\n",
        "    b = [a] * n\n",
        "    time.sleep(1)\n",
        "    return b\n",
        "\n",
        "# duplicate ones a million time\n",
        "print('Duplicate ones a thousand times')\n",
        "start_time = time.time()\n",
        "mem_usage=memory_usage((duplicate_ones,(1,), {'n' : int(1e3)}))\n",
        "end_time = time.time()\n",
        "print('Maximum memory usage (in MB): %s' % max(mem_usage))\n",
        "print('Maximum memory usage (in GB): %s' % (max(mem_usage)/1000))\n",
        "print('Time taken (in s): %s' % (end_time-start_time))\n",
        "\n",
        "# duplicate ones 100 million times\n",
        "print('Duplicate ones a million time')\n",
        "start_time = time.time()\n",
        "mem_usage=memory_usage((duplicate_ones,(1,), {'n' : int(1e8)}))\n",
        "end_time = time.time()\n",
        "print('Maximum memory usage (in MB): %s' % max(mem_usage))\n",
        "print('Maximum memory usage (in GB): %s' % (max(mem_usage)/1000))\n",
        "print('Time taken (in s): %s' % (end_time-start_time))\n",
        "\n",
        "print('Do you notice the difference in time and memory due to the change in duplication size?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating CPU count needs\n",
        "\n",
        "How to evaluate whether our job will benefit from having more CPU available to them?\n",
        "If you don't know whether your function use parallelization or not, because you relies on high-level toolboxes, then you can evaluate that empirically by looking at the time your jobs take depending on the number of CPUs you allow.\n",
        "\n",
        "Let's try first with our last function. It should take about 10s to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import submitit\n",
        "\n",
        "# these commands may not be necessary but helped overcoming an error initially\n",
        "os.environ['SLURM_CPUS_PER_TASK'] = str(1)\n",
        "os.environ['SLURM_TRES_PER_TASK'] = os.environ['SLURM_CPUS_PER_TASK']\n",
        "    \n",
        "# cpu counts to test\n",
        "nCPUs_totest=[1, 4]\n",
        "\n",
        "# loop over cpu counts\n",
        "jcount=0\n",
        "joblist=[]\n",
        "start_time = time.time()\n",
        "for i, cpus in enumerate(nCPUs_totest):\n",
        "    executor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n",
        "    executor.update_parameters(mem_gb=4, timeout_min=5, slurm_partition=\"CPU\", cpus_per_task=cpus)\n",
        "    job = executor.submit(duplicate_ones, 1, int(1e8))\n",
        "    job.n_cpus=cpus\n",
        "    print(\"job with \" + str(job.n_cpus) + \" cpus submitted\")\n",
        "    joblist.append(job)\n",
        "    jcount=jcount+1\n",
        "\n",
        "# wait for job completion\n",
        "njobs_finished = sum(job.done() for job in joblist)\n",
        "while njobs_finished<jcount:\n",
        "    doneIdx=-1\n",
        "    time.sleep(1)\n",
        "    for j, job in enumerate(joblist):\n",
        "        if job.done():\n",
        "            doneIdx=j\n",
        "            break\n",
        "    if doneIdx>=0:\n",
        "        print(str(njobs_finished)+' on ' + str(jcount))\n",
        "        # report last job finished\n",
        "        print(\"job with \" + str(job.n_cpus) + \" cpus returned in \" + str(time.time()-start_time) + \" seconds\")\n",
        "        joblist.pop(doneIdx)\n",
        "        njobs_finished=njobs_finished+1\n",
        "\n",
        "print('### Do you think that increasing the number of CPUs made a big difference? ###')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's redo exactly the same thing, with with a numpy function may benefit from multiple CPUs (i.e. np.dot)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def mat_multiply(size):\n",
        "  # Generate large random matrices\n",
        "  A = np.random.rand(size, size)\n",
        "  B = np.random.rand(size, size)\n",
        "\n",
        "  # Measure time for matrix multiplication\n",
        "  C = np.dot(A, B)\n",
        "  \n",
        "  return 'this function does not return anything special'\n",
        "  \n",
        "os.environ['SLURM_CPUS_PER_TASK'] = str(1)\n",
        "os.environ['SLURM_TRES_PER_TASK'] = os.environ['SLURM_CPUS_PER_TASK']\n",
        "\n",
        "# cpu counts to test\n",
        "nCPUs_totest=[4, 4, 4, 1]\n",
        "\n",
        "# define the max number of jobs that may run in parallel\n",
        "maxjobs=2\n",
        "\n",
        "# loop over cpu counts\n",
        "jcount=0\n",
        "joblist=[]\n",
        "start_time = time.time()\n",
        "for i, cpus in enumerate(nCPUs_totest):\n",
        "    executor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n",
        "    executor.update_parameters(mem_gb=4, timeout_min=5, slurm_partition=\"CPU\", cpus_per_task=cpus)\n",
        "    # check how many job are running (not done) and wait it they exceed our limit\n",
        "    while sum(not job.done() for job in joblist)>maxjobs:\n",
        "        print('wait to submit new job')\n",
        "        time.sleep(3)\n",
        "    job = executor.submit(mat_multiply, 8000)\n",
        "    time.sleep(0.5)\n",
        "    job.n_cpus=cpus\n",
        "    print(\"job with \" + str(job.n_cpus) + \" cpus submitted\")\n",
        "    joblist.append(job)\n",
        "    jcount=jcount+1\n",
        "\n",
        "# wait for job completion\n",
        "njobs_finished = 0; \n",
        "while njobs_finished<jcount:\n",
        "    doneIdx=-1\n",
        "    time.sleep(1)\n",
        "    for j, job in enumerate(joblist):\n",
        "        if job.done():\n",
        "            doneIdx=j\n",
        "            break\n",
        "    if doneIdx>=0:\n",
        "        print(str(njobs_finished)+' on ' + str(jcount))\n",
        "        # report last job finished and print stats\n",
        "        print(\"job with \" + str(job.n_cpus) + \" cpus returned in \" + str(time.time()-start_time) + \" seconds\")\n",
        "        print(\"job status: \" + job.state)\n",
        "        joblist.pop(doneIdx)\n",
        "        njobs_finished=njobs_finished+1\n",
        "\n",
        "print('\\n### Do you think that increasing the number of CPUs made a big difference? ###')\n",
        "print('\\n### MaxRSS indicates the memory used ###')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scaling up **responsibly**\n",
        "\n",
        "In the loop above, you might have noticed something new: we've implemented another good practice by self-limiting the number of jobs we will run in parallel on the cluster. Indeed, it might be ok to launch 40 or even 100 parallel jobs if you are in a hurry, but the amount of CPUs in the cluster is not infinite, and neither is the amount of memory.\n",
        "\n",
        "**Number of CPUs**: you can get this information by running `sinfo -o%C` in your terminal, or `!sinfo -o%C` in the notebook. The CPU partitions have about 350 cores available at the time of writing\n",
        "**Amount of memory**: you can see this by running `sinfo -o \"%P %n %m\"` in your terminal (or with a ! in the notebook). The CPU partitions have about 2.3TB of memory at the time of writing.\n",
        "\n",
        "If it is a sunday and nobody is using the cluster, it is probably fine to increase `maxjobs` to 100 or more (note that if you require 4 cpu per task, it means that you are actually requiring 400 cpus overall!). But if it is 10.30pm on a tuesday, using this parameter might be the same as walking to the coffee machine and taking all the coffee reserves to your office! So, take the habit of setting your `maxjobs`-like parameter on a daily basis after checking `sinfo -o%C`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check node and CPU information\n",
        "print(\"### Node counts: \\nA: currently in use \\B available\")\n",
        "!sinfo -o%A\n",
        "print(\"### CPU counts: \\nA: core you currently use (notebook) \\nI: available \\nO: unavailable (maintenance, down, etc) \\nT: total\")\n",
        "!sinfo -o%C\n",
        "\n",
        "# check some stats of our last job\n",
        "print('### CPU time and MaxRSS of our last job (about 1Gb should be added to your MaxRSS in order to cover safely the memory needs of the python runtime)###')\n",
        "os.system(f'sacct -j {job.job_id} --format=\"CPUTime,MaxRSS\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A more compact approach\n",
        "\n",
        "In the above examples, we have decomposed most operations using for loops in order to illustrate the different concepts. But with more advanced methods we can compact a lot the code used above.\n",
        "\n",
        "The example below (taken from [submitit documentation](https://github.com/facebookincubator/submitit/blob/main/docs/examples.md)) allows getting rid of the job submission loop and directly map our input arrays to job submissions, using executor.map_array and some asynchronous operations.\n",
        "Note that such compact approach might be more difficult to debug."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "# just add a/b, multiply by c and wait for b seconds\n",
        "def simple_function(a, b, c):\n",
        "    output=(a + b)*c\n",
        "    time.sleep(b)\n",
        "    return output\n",
        "\n",
        "# define arrays matched in length for the iteration (if you have constant parameters, you can always duplicate them as done with \"c\" below)\n",
        "a = [1, 2, 2, 1, 0, 1]\n",
        "b = [10, 20, 30, 40, 30, 10]\n",
        "c=[0.1]*len(b)\n",
        "\n",
        "# make sure our arrays are matched in length\n",
        "assert len(a)==len(b)==len(c)\n",
        "\n",
        "# prepare executor\n",
        "executor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n",
        "\n",
        "# define maxjobs to a low value to illustrate\n",
        "maxjobs=3\n",
        "\n",
        "# the pupdate_parameters(slurm_array_parallelism=maxjobs, mem_gb=2, timeout_min=4, slurm_partition=\"CPU\", cpus_per_task=1)\n",
        "\n",
        "# execute the job (note the .map_array command that different from the .submit command used above)\n",
        "jobs = executor.map_array(simple_function, a, b, c)  # just a list of jobs\n",
        "\n",
        "# print results as they become available\n",
        "for aws in asyncio.as_completed([j.awaitable().result() for j in jobs]):\n",
        "    result = await aws\n",
        "    print(\"result of computation: \" + str(result))\n",
        "    arameter \"slurm_array_parallelism\" tells submitit to limit the number of concurrent jobs\n",
        "executor.\n",
        "# note that we use here an asynchronous method based on asyncio\n",
        "# it essential do something similar to what we were doing after \n",
        "# \"# wait for job completion\", but in a much more compact way\n",
        "# however, the reordering of outputs wrt to inputs is not implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Submitting and going home\n",
        "\n",
        "Often, when we have very long jobs, we want to submit these jobs, go home and come back the next day or the next week to check the results of their computations.\n",
        "\n",
        "In this case, we **should not** expect our notebook to be still alive when we come back. Instead, we should adopt the more standard approach of writing down our results and load them in a new jupyter session afterwards!\n",
        "\n",
        "This is way will we simulate in the final example below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# write in job_output within our home directory (~ is synonymous of /home/username/)\n",
        "job_output_folder=os.getcwd()+'/tuto_output/'\n",
        "\n",
        "# make sure our output folder exists\n",
        "if not os.path.exists(job_output_folder):\n",
        "  os.makedirs(job_output_folder)\n",
        "\n",
        "# just add a/b, multiply by c, wait for b seconds and write down the result to an output folder (c)\n",
        "def simple_function_write(a, b, c):\n",
        "    output=(a + b)\n",
        "    time.sleep(b)\n",
        "    output_filepath=os.path.join(c, str(a) + '_' + str(b) + '.txt')\n",
        "    with open(output_filepath, 'w') as file:\n",
        "      file.write(f'{a}\\n')\n",
        "      file.write(f'{b}\\n')\n",
        "    \n",
        "# define arrays matched in length for the iteration (if you have constant parameters, you can always duplicate them as done with \"c\" below)\n",
        "a = [1, 2, 2, 1, 0, 1]\n",
        "b = [10, 20, 30, 40, 30, 10]\n",
        "c=[job_output_folder]*len(b)\n",
        "\n",
        "# make sure our arrays are matched in length\n",
        "assert len(a)==len(b)==len(c)\n",
        "\n",
        "# prepare executor\n",
        "executor = submitit.AutoExecutor(folder=\"joblogs\")\n",
        "\n",
        "# define maxjobs to a low value to illustrate\n",
        "maxjobs=3\n",
        "\n",
        "# the pupdate_parameters(slurm_array_parallelism=maxjobs, mem_gb=2, timeout_min=4, slurm_partition=\"CPU\", cpus_per_task=1)\n",
        "\n",
        "# execute the job (note the .map_array command that different from the .submit command used above)\n",
        "jobs = executor.map_array(simple_function_write, a, b, c)  # just a list of jobs\n",
        "print('### all jobs submitted ###')\n",
        "print('the kernel will now be killed (and your notebook will crash) but you can see that your jobs keep running by typing squeue in the terminal')\n",
        "\n",
        "# wait a little and kill manually the kernel process\n",
        "time.sleep(3)\n",
        "os.system('kill ' + str(os.getpid()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Whether you need several CPUs, and how to set memory and timeout parameters depend on the functions you use. \n",
        "\n",
        "If you are not sure, look in the documentation of your packages or test for a performance improvement as we just did!\n",
        "\n",
        "Im\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3",
      "path": "C:\\Users\\romai\\AppData\\Roaming\\Python\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
